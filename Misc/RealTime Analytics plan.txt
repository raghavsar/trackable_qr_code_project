


Based on my analysis, I'll provide a detailed plan to enhance your analytics system with real-time capabilities. Here's the implementation plan:

# Real-Time Analytics Enhancement Plan

## 1. Current System Analysis

Your current system has:
- Analytics service microservice
- MongoDB for data storage
- Frontend dashboard at `/analytics`
- Basic tracking implementation

## 2. Proposed Architecture

```mermaid
graph TD
    Client[QR Code Scanner] -->|Scan Event| Gateway[API Gateway]
    Gateway -->|Event| Analytics[Analytics Service]
    Analytics -->|Raw Data| MongoDB[(MongoDB)]
    Analytics -->|Real-time Counters| Redis[(Redis)]
    Analytics -->|WebSocket| Dashboard[Analytics Dashboard]
    CronJob[Aggregation Cron] -->|Daily Rollup| MongoDB
```

## 3. Implementation Plan

### 3.1 WebSocket Server Implementation (FastAPI)
```python
# analytics-service/app/websocket.py
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from typing import List, Dict
import asyncio
import json

class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []
        self.last_metrics: Dict = {}

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)
        # Send latest metrics immediately upon connection
        await websocket.send_json(self.last_metrics)

    async def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)

    async def broadcast_metrics(self, metrics: dict):
        self.last_metrics = metrics
        for connection in self.active_connections:
            try:
                await connection.send_json(metrics)
            except WebSocketDisconnect:
                await self.disconnect(connection)

manager = ConnectionManager()

# In your FastAPI app
@app.websocket("/ws/analytics")
async def websocket_endpoint(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        while True:
            await websocket.receive_text()  # Keep connection alive
    except WebSocketDisconnect:
        await manager.disconnect(websocket)
```

### 3.2 Redis Integration for Real-Time Counters
```python
# analytics-service/app/redis_client.py
from redis import Redis
from datetime import datetime
import json

class AnalyticsRedis:
    def __init__(self):
        self.redis = Redis(
            host='redis',
            port=6379,
            decode_responses=True
        )
        
    async def increment_scan_counter(self, qr_id: str):
        """Increment scan counters in various time windows"""
        now = datetime.utcnow()
        pipe = self.redis.pipeline()
        
        # Increment total scans
        pipe.incr(f"total_scans:{qr_id}")
        
        # Increment hourly counter
        hourly_key = f"hourly_scans:{qr_id}:{now.strftime('%Y-%m-%d-%H')}"
        pipe.incr(hourly_key)
        pipe.expire(hourly_key, 86400)  # Expire after 24 hours
        
        # Increment daily counter
        daily_key = f"daily_scans:{qr_id}:{now.strftime('%Y-%m-%d')}"
        pipe.incr(daily_key)
        pipe.expire(daily_key, 604800)  # Expire after 7 days
        
        await pipe.execute()

    async def get_real_time_metrics(self):
        """Get current metrics for dashboard"""
        # Implement your metrics aggregation logic here
        pass
```

### 3.3 MongoDB Raw Data Schema
```python
# analytics-service/app/models.py
from datetime import datetime
from pydantic import BaseModel
from typing import Optional, Dict

class ScanEvent(BaseModel):
    qr_id: str
    timestamp: datetime
    user_agent: str
    ip_address: str
    location: Optional[Dict[str, float]]
    referrer: Optional[str]
    device_info: Dict[str, str]
    scan_duration: Optional[float]

class DailyAggregate(BaseModel):
    date: datetime
    qr_id: str
    total_scans: int
    unique_visitors: int
    avg_scan_duration: float
    device_breakdown: Dict[str, int]
    location_breakdown: Dict[str, int]
    hourly_distribution: Dict[str, int]
```

### 3.4 Cron Job for Daily Aggregation
```python
# analytics-service/app/tasks.py
from datetime import datetime, timedelta
from motor.motor_asyncio import AsyncIOMotorClient
from .models import DailyAggregate

async def aggregate_daily_stats():
    """Daily aggregation of scan events"""
    yesterday = datetime.utcnow() - timedelta(days=1)
    start_of_day = yesterday.replace(hour=0, minute=0, second=0, microsecond=0)
    end_of_day = start_of_day + timedelta(days=1)

    pipeline = [
        {
            "$match": {
                "timestamp": {
                    "$gte": start_of_day,
                    "$lt": end_of_day
                }
            }
        },
        {
            "$group": {
                "_id": "$qr_id",
                "total_scans": {"$sum": 1},
                "unique_visitors": {"$addToSet": "$ip_address"},
                "scan_durations": {"$push": "$scan_duration"},
                "devices": {"$push": "$device_info.device_type"},
                "locations": {"$push": "$location.country"},
                "hourly_scans": {
                    "$push": {
                        "$hour": "$timestamp"
                    }
                }
            }
        }
    ]

    # Execute aggregation and store results
```

### 3.5 Frontend WebSocket Integration
```typescript
// frontend/src/hooks/useAnalytics.ts
import { useState, useEffect } from 'react';
import { useWebSocket } from './useWebSocket';

export const useAnalytics = () => {
  const [metrics, setMetrics] = useState<AnalyticsMetrics | null>(null);
  const ws = useWebSocket('ws://localhost:8004/ws/analytics');

  useEffect(() => {
    if (!ws) return;

    ws.onmessage = (event) => {
      const data = JSON.parse(event.data);
      setMetrics(data);
    };

    return () => {
      ws.close();
    };
  }, [ws]);

  return metrics;
};
```

### 3.6 Docker Configuration Updates
```yaml
# In docker-compose.yml
services:
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - qr-network

  analytics-service:
    # ... existing config ...
    depends_on:
      - redis
    environment:
      - REDIS_URL=redis://redis:6379
```

## 4. Implementation Steps

1. **Set Up Redis**
   - Add Redis service to docker-compose
   - Configure Redis client in analytics service
   - Implement counter logic

2. **WebSocket Implementation**
   - Add WebSocket endpoints to analytics service
   - Implement connection manager
   - Add real-time broadcast logic

3. **MongoDB Schema Updates**
   - Implement new schema for raw scan data
   - Add indexes for efficient querying
   - Set up TTL for raw data

4. **Cron Job Setup**
   - Implement daily aggregation logic
   - Set up cron schedule
   - Add error handling and logging

5. **Frontend Updates**
   - Add WebSocket connection handling
   - Update dashboard components
   - Implement real-time updates

## 5. Error Prevention and Best Practices

1. **WebSocket Handling**
   - Implement reconnection logic
   - Handle connection timeouts
   - Add heartbeat mechanism
   - Implement proper error handling

2. **Redis Best Practices**
   - Use pipeline for batch operations
   - Implement proper key expiration
   - Handle connection failures
   - Use appropriate data structures

3. **Cron Job Reliability**
   - Implement job locking
   - Add retry mechanism
   - Log job execution status
   - Handle partial failures

4. **Data Consistency**
   - Implement data validation
   - Add data cleanup jobs
   - Handle timezone issues
   - Implement data backfill mechanism

Would you like me to provide more detailed documentation for any specific component or proceed with implementing a particular part of this plan?
